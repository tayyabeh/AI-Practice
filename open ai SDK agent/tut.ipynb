{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97253aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai-agents\n",
    "%pip install openai-agents[litellm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30beda46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
    "print(os.getenv(\"GEMINI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d0721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent ,Runner\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00de151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     agent = Agent (name= \"Tester\" , instructions=\"Answer the question \", model=\"litellm/gemini/gemini-1.5-flash\")\n",
    "\n",
    "#     result = await Runner.run(agent , \"write a motivational quote of work\")\n",
    "#     print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a93b3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner\n",
    "\n",
    "# creating an agent \n",
    "agent = Agent(\n",
    "    name = \"Gemini agent\",\n",
    "    instructions = \"You are a helpfull assistant to guide the user query \",\n",
    "    model = \"litellm/gemini/gemini-1.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b6fe8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    result = await Runner.run(agent,input=\"Can you tell me what is the capital of Pakistan?\")\n",
    "    print(\"Agent : \" , result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c3273d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\typing.py:407: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  def _eval_type(t, globalns, localns, type_params=None, *, recursive_guard=frozenset()):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent :  The capital of Pakistan is **Islamabad**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545af8",
   "metadata": {},
   "source": [
    "# Basic configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ceff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent , Runner\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Helpfull Assistant\",\n",
    "    instructions = \"You are the helpful assistant\",\n",
    "    model = \"litellm/gemini/gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "# result = Runner.run_sync(agent , input=\"What is the capital of France?\")\n",
    "# print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522ba36",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376e9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent , Runner , function_tool\n",
    "import os \n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "78f02e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
    "print(os.getenv(\"GEMINI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9364c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Context for shopping assistant\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ShoppingAssistantContext:\n",
    "    name: str \n",
    "    age: int\n",
    "    phoneno: int\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bafcc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "comtext = ShoppingAssistantContext(\n",
    "    name = \"Tayyab ELlahi\",\n",
    "    age = 20,\n",
    "    phoneno= 33344455566,\n",
    "    email= \"tayyabeh1807@gmail.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic instruction\n",
    "def dynamic_instruction(comtext , agent):\n",
    "    return f\"You are a helpful shopping assistant. The user's name is {comtext.name}. Greet the user by their name and ask how you can help them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b5eb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an agent\n",
    "agent = Agent(\n",
    "    name = \"Shopping Assistant Agent\",\n",
    "    instructions = dynamic_instruction,\n",
    "    model = \"litellm/gemini/gemini-1.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea1ea9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tayyab Ellahi,\n",
      "\n",
      "It's great to hear from you!  I'd be happy to help you with your shopping for an iPhone 15. To best assist you, could you tell me a little more about what you're looking for?  For example:\n",
      "\n",
      "* **Which model of the iPhone 15 are you interested in?** (iPhone 15, iPhone 15 Plus, iPhone 15 Pro, iPhone 15 Pro Max)\n",
      "* **What's your budget?**\n",
      "* **Where are you planning to buy it from?** (e.g., Apple Store, carrier, online retailer)\n",
      "* **Are there any specific features that are important to you?** (e.g., camera quality, storage capacity, battery life)\n",
      "\n",
      "The more information you give me, the better I can assist you in finding the perfect iPhone 15 for your needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# main \n",
    "async def main():\n",
    "    result = await Runner.run(agent , input = \"Plz help me in shopping..i want obuy a phone iphone 15\",context = comtext)\n",
    "    print(result.final_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c95243",
   "metadata": {},
   "source": [
    "# Output Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1f08cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "from pydantic import BaseModel, EmailStr\n",
    "import asyncio\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9860d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    phoneno : str\n",
    "    email: EmailStr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31001001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e5f1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name = \"Extract Infor Agent\", \n",
    "    instructions = \"You are a helpfull assistant that can extract user infor from user input\",\n",
    "    model = \"litellm/gemini/gemini-1.5-flash\",\n",
    "    output_type = ContactInfo    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98881964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='tayyab ellahi' phoneno='9233345434124' email='tayyab@gmail.com'\n"
     ]
    }
   ],
   "source": [
    "user_input = 'hello i am tayyab ellahi and i m a passionate ai engineer and you can contact me through email tayyab@gmail.com and also contact me from phone 9233345434124'\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent,user_input)\n",
    "    print(result.final_output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd12e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5938bf6",
   "metadata": {},
   "source": [
    "# HandsOff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b0fab",
   "metadata": {},
   "source": [
    "GUARDRAIL IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d158a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent , Runner, handoff, RunContextWrapper, HandoffInputData, GuardrailFunctionOutput , InputGuardrailTripwireTriggered, OutputGuardrailTripwireTriggered, input_guardrail,  output_guardrail\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from pydantic import BaseModel, EmailStr\n",
    "import asyncio\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80a8df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyDQ5UCL6CHe_-FbtSkDDuvpUE8mrF_FCTg\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDQ5UCL6CHe_-FbtSkDDuvpUE8mrF_FCTg\"\n",
    "print(os.getenv(\"GEMINI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21b14c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91649465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response output agent \n",
    "# -------- Pydantic Schemas --------\n",
    "class CustomerSupportOutput(BaseModel):\n",
    "    is_customer_output: bool\n",
    "    reasoning: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ca0cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageOutput(BaseModel):\n",
    "    user_question: str\n",
    "    response: str\n",
    "    problem_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3fc0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalProblem(BaseModel):\n",
    "    product_id : str\n",
    "    description : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5285587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BillingProblem(BaseModel):\n",
    "    invoice_no : str\n",
    "    problem_type : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afea59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerSupportOutput(BaseModel):\n",
    "    is_customer_output: bool\n",
    "    reasoning : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "594fd2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_agent = Agent(\n",
    "    name = \"Guardrial Check\",\n",
    "    instructions= '''\n",
    "    You are a guardrail. Your job is to check if the user's query is related to **customer support**.\n",
    "\n",
    "    Customer support topics include:\n",
    "    - Billing problems\n",
    "    - Technical problems\n",
    "\n",
    "    If the query is related to customer support → set `is_customer_output=true`.\n",
    "    Otherwise (like math questions, chit-chat, unrelated queries) → set `is_customer_output=false`.\n",
    "\n",
    "    Always respond in valid JSON with fields:\n",
    "    {\n",
    "      \"is_customer_output\": true/false,\n",
    "      \"reasoning\": \"your short explanation\"\n",
    "    }\n",
    "    ''',\n",
    "    model = \"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type = CustomerSupportOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "671ec1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Output Guardrail Agent --------\n",
    "guardrail_output_agent = Agent(\n",
    "    name=\"Guardrail Output Check\",\n",
    "    instructions=\"\"\"\n",
    "    You are a guardrail. Check if the agent's output is valid.\n",
    "\n",
    "    Valid output must be JSON with exactly:\n",
    "    {\n",
    "      \"user_question\": \"...\",\n",
    "      \"response\": \"...\",\n",
    "      \"problem_type\": \"billing\" or \"technical\"\n",
    "    }\n",
    "\n",
    "    If it's valid → set `is_customer_output=true`.\n",
    "    If it's invalid (missing fields, wrong format, or problem_type not in [billing,technical]) → set `is_customer_output=false`.\n",
    "    \"\"\",\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type=CustomerSupportOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23c68e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@input_guardrail\n",
    "async def customer_guardrail(ctx: RunContextWrapper , agent :Agent, input:str) -> GuardrailFunctionOutput:\n",
    "    result = await Runner.run(guardrail_agent, input , context = ctx.context)\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info = result.final_output,\n",
    "        tripwire_triggered = not result.final_output.is_customer_output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6e8f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output_guardrail\n",
    "async def customer_output_guardrail(ctx: RunContextWrapper, agent: Agent, output: MessageOutput) -> GuardrailFunctionOutput:\n",
    "    result = await Runner.run(guardrail_output_agent, output.dict(), context=ctx.context)\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=result.final_output,\n",
    "        tripwire_triggered=not result.final_output.is_customer_output  # ❌ fail if not valid format\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b7947fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function input filter function \n",
    "def billing_input_filter(input:HandoffInputData):   \n",
    "#     if len(problem.invoice_no) != 6 :\n",
    "#         raise ValueError(\"your invoice must be 6 digit number\")\n",
    "    print(input.input_history)\n",
    "    # Nested dictionary aur list se 'arguments' ki value tak pohunchna\n",
    "    arguments_value = input['HandoffInputData']['new_items'][0]['raw_item']['arguments']\n",
    "    \n",
    "    # Value ko print karna\n",
    "    print(arguments_value)\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c540b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Agents SDK ke zaroori imports yahan honge (farz kar lein)\n",
    "# from agents.runtime import HandoffInputData \n",
    "\n",
    "def billing_input_filter(input: HandoffInputData):\n",
    "    \"\"\"\n",
    "    Yeh filter HandoffInputData object se invoice number nikalta hai\n",
    "    aur uski length ko validate karta hai.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Object ke attributes ko dot (.) se access karein\n",
    "        # 'new_items' ek list/tuple hai, isliye pehla item [0] se lein.\n",
    "        # 'raw_item' ek aur object hai, jiske andar 'arguments' hai.\n",
    "        arguments_string = input.new_items[0].raw_item.arguments\n",
    "        \n",
    "        # 2. 'arguments' ek JSON string hai. Ise Python dictionary mein convert karein.\n",
    "        arguments_dict = json.loads(arguments_string)\n",
    "        \n",
    "        # 3. Dictionary se 'invoice_no' nikalein.\n",
    "        invoice_no = arguments_dict['invoice_no']\n",
    "\n",
    "        print(f\"Invoice number mila: {invoice_no}\") # Debugging ke liye\n",
    "\n",
    "        # 4. Ab aap apna validation logic chala sakte hain.\n",
    "        if len(invoice_no) != 6:\n",
    "            # Agar validation fail ho, to error raise karein. SDK isko handle karega.\n",
    "            raise ValueError(\"Aapka invoice 6 digit ka hona chahiye.\")\n",
    "        \n",
    "        print(\"Invoice number validation pass ho gaya!\")\n",
    "\n",
    "    except (IndexError, KeyError, AttributeError) as e:\n",
    "        # Agar invoice number nahi milta to error raise karein\n",
    "        raise ValueError(f\"Invoice number input data mein nahi mil saka. Error: {e}\")\n",
    "\n",
    "    # Agar sab kuch theek hai, to original input ko wapas return karein.\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0af171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3ebbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_handoff_callback(ctx : RunContextWrapper , input: TechnicalProblem):\n",
    "    print( \"Yes the technical Agent is called!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c93dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_support_agent= Agent(\n",
    "    name = \"Technical Support Agent\",\n",
    "    instructions = f'''\n",
    "    {RECOMMENDED_PROMPT_PREFIX} \n",
    "    You are a helpfull Technical Support  assistant and your work is to address with user techinical queries\n",
    "    ''',\n",
    "    model = \"litellm/gemini/gemini-2.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a651159",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_support_handoff = handoff(\n",
    "    agent = technical_support_agent,\n",
    "    tool_name_override = 'Transfer_to_Techinacal_support',\n",
    "    tool_description_override = \"Transfers a request to the technical support team.\",\n",
    "    on_handoff = on_handoff_callback,\n",
    "    input_type = TechnicalProblem\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd0a4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_agent= Agent(\n",
    "    name = \"Billing Agent\",\n",
    "    instructions = f'''\n",
    "    {RECOMMENDED_PROMPT_PREFIX} \n",
    "    You are a helpfull billing assistant agent and your work is to addres with user bills\n",
    "    ''',\n",
    "    model = \"litellm/gemini/gemini-2.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "933b4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_handoff_callback_billing(ctx : RunContextWrapper , input_data: BillingProblem):\n",
    "    print(f\"Yes the billing Agent is called!! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2b3580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_handoff = handoff(\n",
    "    agent = billing_agent,    \n",
    "    on_handoff = on_handoff_callback_billing,\n",
    "    input_type = BillingProblem,\n",
    "    input_filter = billing_input_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21baf98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "triage_agent= Agent(\n",
    "    name = \"Triage Agent\",\n",
    "    instructions=\"\"\"\n",
    "    {RECOMMENDED_PROMPT_PREFIX}\n",
    "    You are a helpful triage assistant. Your main job is to carefully read the user's query \n",
    "    and route it to the correct support team. \n",
    "    \n",
    "    Rules:\n",
    "    - If the user describes a **technical problem** \n",
    "      (examples: product not working, error messages, bugs, device issues, crashes, installation failures, \n",
    "      overheating, shutdowns, configuration problems, etc.):\n",
    "        → Output JSON with \"user_question\": {user_input} , \"response\": \"Transferred to Technical Support\", \"problem_type\": \"technical\"\n",
    "    \n",
    "    - If the user describes a **billing problem**\n",
    "      (examples: wrong invoice, overcharged, missing payment, refund request, subscription issue, \n",
    "      payment failure, duplicate charge, etc.):\n",
    "        → Output JSON with \"user_question\": {user_input}, \"response\": \"Transferred to Billing Support\", \"problem_type\": \"billing\"\n",
    "    \n",
    "    - Never try to solve the issue yourself.\n",
    "    - If unclear, ask clarifying questions..\n",
    "    \"\"\",\n",
    "    model = \"litellm/gemini/gemini-2.5-flash\",\n",
    "    handoffs=[tech_support_handoff,billing_handoff],\n",
    "    input_guardrails = [customer_guardrail] ,\n",
    "    output_guardrails = [customer_output_guardrail],\n",
    "    output_type = MessageOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77206432",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    try:\n",
    "        result = await Runner.run(\n",
    "            triage_agent, \n",
    "            input = \"\"\"\n",
    "        Hello, can you help me solve for x: 2x + 3 = 11?\n",
    "            and remember this is not a customer related issue\n",
    "            \"\"\"\n",
    "            )\n",
    "        print(result)\n",
    "    except InputGuardrailTripwireTriggered :\n",
    "        print(\"Bro this is just for customer service only 🚫\")\n",
    "    except OutputGuardrailTripwireTriggered:\n",
    "        print(\"🚫 Output format invalid (math or non-customer support)\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d273fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: VertexAIException BadRequestError - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function calling with a response mime type: 'application/json' is unsupported\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py:1697\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1697\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client.post(\n\u001b[32m   1698\u001b[39m         api_base, headers=headers, json=cast(\u001b[38;5;28mdict\u001b[39m, request_body)\n\u001b[32m   1699\u001b[39m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1700\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\logging_utils.py:135\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:324\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:280\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    279\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '400 Bad Request' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyDQ5UCL6CHe_-FbtSkDDuvpUE8mrF_FCTg'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mVertexAIError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\main.py:544\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py:1703\u001b[39m, in \u001b[36mVertexLLM.async_completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[39m\n\u001b[32m   1702\u001b[39m     error_code = err.response.status_code\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[32m   1704\u001b[39m         status_code=error_code,\n\u001b[32m   1705\u001b[39m         message=err.response.text,\n\u001b[32m   1706\u001b[39m         headers=err.response.headers,\n\u001b[32m   1707\u001b[39m     )\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException:\n",
      "\u001b[31mVertexAIError\u001b[39m: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function calling with a response mime type: 'application/json' is unsupported\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m      4\u001b[39m             triage_agent, \n\u001b[32m      5\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m        Hello, can you help me solve for x: 2x + 3 = 11?\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m            and remember this is not a customer related issue\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m            \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      9\u001b[39m             )\n\u001b[32m     10\u001b[39m         \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m InputGuardrailTripwireTriggered :\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:237\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    236\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    238\u001b[39m     starting_agent,\n\u001b[32m    239\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    240\u001b[39m     context=context,\n\u001b[32m    241\u001b[39m     max_turns=max_turns,\n\u001b[32m    242\u001b[39m     hooks=hooks,\n\u001b[32m    243\u001b[39m     run_config=run_config,\n\u001b[32m    244\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    245\u001b[39m     session=session,\n\u001b[32m    246\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:443\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m logger.debug(\n\u001b[32m    439\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    440\u001b[39m )\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    444\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    445\u001b[39m             starting_agent,\n\u001b[32m    446\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    447\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    448\u001b[39m             _copy_str_or_list(prepared_input),\n\u001b[32m    449\u001b[39m             context_wrapper,\n\u001b[32m    450\u001b[39m         ),\n\u001b[32m    451\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    452\u001b[39m             agent=current_agent,\n\u001b[32m    453\u001b[39m             all_tools=all_tools,\n\u001b[32m    454\u001b[39m             original_input=original_input,\n\u001b[32m    455\u001b[39m             generated_items=generated_items,\n\u001b[32m    456\u001b[39m             hooks=hooks,\n\u001b[32m    457\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    458\u001b[39m             run_config=run_config,\n\u001b[32m    459\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    460\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    461\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    462\u001b[39m         ),\n\u001b[32m    463\u001b[39m     )\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    465\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    466\u001b[39m         agent=current_agent,\n\u001b[32m    467\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    475\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:1047\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1045\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1048\u001b[39m     agent,\n\u001b[32m   1049\u001b[39m     system_prompt,\n\u001b[32m   1050\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1051\u001b[39m     output_schema,\n\u001b[32m   1052\u001b[39m     all_tools,\n\u001b[32m   1053\u001b[39m     handoffs,\n\u001b[32m   1054\u001b[39m     context_wrapper,\n\u001b[32m   1055\u001b[39m     run_config,\n\u001b[32m   1056\u001b[39m     tool_use_tracker,\n\u001b[32m   1057\u001b[39m     previous_response_id,\n\u001b[32m   1058\u001b[39m     prompt_config,\n\u001b[32m   1059\u001b[39m )\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1062\u001b[39m     agent=agent,\n\u001b[32m   1063\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1072\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1073\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:1275\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, prompt_config)\u001b[39m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.hooks:\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m agent.hooks.on_llm_start(\n\u001b[32m   1269\u001b[39m         context_wrapper,\n\u001b[32m   1270\u001b[39m         agent,\n\u001b[32m   1271\u001b[39m         filtered.instructions,  \u001b[38;5;66;03m# Use filtered instructions\u001b[39;00m\n\u001b[32m   1272\u001b[39m         filtered.input,  \u001b[38;5;66;03m# Use filtered input\u001b[39;00m\n\u001b[32m   1273\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1276\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1278\u001b[39m     model_settings=model_settings,\n\u001b[32m   1279\u001b[39m     tools=all_tools,\n\u001b[32m   1280\u001b[39m     output_schema=output_schema,\n\u001b[32m   1281\u001b[39m     handoffs=handoffs,\n\u001b[32m   1282\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1283\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1284\u001b[39m     ),\n\u001b[32m   1285\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1286\u001b[39m     prompt=prompt_config,\n\u001b[32m   1287\u001b[39m )\n\u001b[32m   1288\u001b[39m \u001b[38;5;66;03m# If the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n\u001b[32m   1289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\extensions\\models\\litellm_model.py:94\u001b[39m, in \u001b[36mLitellmModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     78\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m     prompt: Any | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     87\u001b[39m ) -> ModelResponse:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     89\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     90\u001b[39m         model_config=model_settings.to_json_dict()\n\u001b[32m     91\u001b[39m         | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mmodel_impl\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     92\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     93\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     95\u001b[39m             system_instructions,\n\u001b[32m     96\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     97\u001b[39m             model_settings,\n\u001b[32m     98\u001b[39m             tools,\n\u001b[32m     99\u001b[39m             output_schema,\n\u001b[32m    100\u001b[39m             handoffs,\n\u001b[32m    101\u001b[39m             span_generation,\n\u001b[32m    102\u001b[39m             tracing,\n\u001b[32m    103\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    104\u001b[39m             prompt=prompt,\n\u001b[32m    105\u001b[39m         )\n\u001b[32m    107\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.choices[\u001b[32m0\u001b[39m], litellm.types.utils.Choices)\n\u001b[32m    109\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\extensions\\models\\litellm_model.py:313\u001b[39m, in \u001b[36mLitellmModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_settings.extra_args:\n\u001b[32m    311\u001b[39m     extra_kwargs.update(model_settings.extra_args)\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m litellm.acompletion(\n\u001b[32m    314\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    315\u001b[39m     messages=converted_messages,\n\u001b[32m    316\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    317\u001b[39m     temperature=model_settings.temperature,\n\u001b[32m    318\u001b[39m     top_p=model_settings.top_p,\n\u001b[32m    319\u001b[39m     frequency_penalty=model_settings.frequency_penalty,\n\u001b[32m    320\u001b[39m     presence_penalty=model_settings.presence_penalty,\n\u001b[32m    321\u001b[39m     max_tokens=model_settings.max_tokens,\n\u001b[32m    322\u001b[39m     tool_choice=\u001b[38;5;28mself\u001b[39m._remove_not_given(tool_choice),\n\u001b[32m    323\u001b[39m     response_format=\u001b[38;5;28mself\u001b[39m._remove_not_given(response_format),\n\u001b[32m    324\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    325\u001b[39m     stream=stream,\n\u001b[32m    326\u001b[39m     stream_options=stream_options,\n\u001b[32m    327\u001b[39m     reasoning_effort=reasoning_effort,\n\u001b[32m    328\u001b[39m     top_logprobs=model_settings.top_logprobs,\n\u001b[32m    329\u001b[39m     extra_headers={**HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    330\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.api_key,\n\u001b[32m    331\u001b[39m     base_url=\u001b[38;5;28mself\u001b[39m.base_url,\n\u001b[32m    332\u001b[39m     **extra_kwargs,\n\u001b[32m    333\u001b[39m )\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, litellm.types.utils.ModelResponse):\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\utils.py:1586\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1584\u001b[39m timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n\u001b[32m   1585\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1586\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\utils.py:1437\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1434\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1436\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1437\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1438\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1440\u001b[39m     kwargs=kwargs,\n\u001b[32m   1441\u001b[39m     call_type=call_type,\n\u001b[32m   1442\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\main.py:563\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    562\u001b[39m     custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2301\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2300\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2303\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:1293\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_exception.status_code == \u001b[32m400\u001b[39m:\n\u001b[32m   1292\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[32m   1294\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVertexAIException BadRequestError - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1295\u001b[39m         model=model,\n\u001b[32m   1296\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33mvertex_ai\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1297\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   1298\u001b[39m         response=httpx.Response(\n\u001b[32m   1299\u001b[39m             status_code=\u001b[32m400\u001b[39m,\n\u001b[32m   1300\u001b[39m             request=httpx.Request(\n\u001b[32m   1301\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1302\u001b[39m                 url=\u001b[33m\"\u001b[39m\u001b[33mhttps://cloud.google.com/vertex-ai/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1303\u001b[39m             ),\n\u001b[32m   1304\u001b[39m         ),\n\u001b[32m   1305\u001b[39m     )\n\u001b[32m   1306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_exception.status_code == \u001b[32m401\u001b[39m:\n\u001b[32m   1307\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: VertexAIException BadRequestError - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function calling with a response mime type: 'application/json' is unsupported\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    await main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796feaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808da768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
