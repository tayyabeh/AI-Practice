{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97253aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai-agents\n",
    "%pip install openai-agents[litellm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30beda46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
    "print(os.getenv(\"GEMINI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d0721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent ,Runner\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00de151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     agent = Agent (name= \"Tester\" , instructions=\"Answer the question \", model=\"litellm/gemini/gemini-1.5-flash\")\n",
    "\n",
    "#     result = await Runner.run(agent , \"write a motivational quote of work\")\n",
    "#     print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a93b3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner\n",
    "\n",
    "# creating an agent \n",
    "agent = Agent(\n",
    "    name = \"Gemini agent\",\n",
    "    instructions = \"You are a helpfull assistant to guide the user query \",\n",
    "    model = \"litellm/gemini/gemini-1.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b6fe8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    result = await Runner.run(agent,input=\"Can you tell me what is the capital of Pakistan?\")\n",
    "    print(\"Agent : \" , result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c3273d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\typing.py:407: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  def _eval_type(t, globalns, localns, type_params=None, *, recursive_guard=frozenset()):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent :  The capital of Pakistan is **Islamabad**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545af8",
   "metadata": {},
   "source": [
    "# Basic configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ceff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent , Runner\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Helpfull Assistant\",\n",
    "    instructions = \"You are the helpful assistant\",\n",
    "    model = \"litellm/gemini/gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "# result = Runner.run_sync(agent , input=\"What is the capital of France?\")\n",
    "# print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522ba36",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376e9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent , Runner , function_tool\n",
    "import os \n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "78f02e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
    "print(os.getenv(\"GEMINI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9364c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Context for shopping assistant\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ShoppingAssistantContext:\n",
    "    name: str \n",
    "    age: int\n",
    "    phoneno: int\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bafcc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "comtext = ShoppingAssistantContext(\n",
    "    name = \"Tayyab ELlahi\",\n",
    "    age = 20,\n",
    "    phoneno= 33344455566,\n",
    "    email= \"tayyabeh1807@gmail.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic instruction\n",
    "def dynamic_instruction(comtext , agent):\n",
    "    return f\"You are a helpful shopping assistant. The user's name is {comtext.name}. Greet the user by their name and ask how you can help them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5eb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an agent\n",
    "agent = Agent(\n",
    "    name = \"Shopping Assistant Agent\",\n",
    "    instructions = dynamic_instruction,\n",
    "    model = \"litellm/gemini/gemini-2.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea1ea9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tayyab Ellahi,\n",
      "\n",
      "It's great to hear from you!  I'd be happy to help you with your shopping for an iPhone 15. To best assist you, could you tell me a little more about what you're looking for?  For example:\n",
      "\n",
      "* **Which model of the iPhone 15 are you interested in?** (iPhone 15, iPhone 15 Plus, iPhone 15 Pro, iPhone 15 Pro Max)\n",
      "* **What's your budget?**\n",
      "* **Where are you planning to buy it from?** (e.g., Apple Store, carrier, online retailer)\n",
      "* **Are there any specific features that are important to you?** (e.g., camera quality, storage capacity, battery life)\n",
      "\n",
      "The more information you give me, the better I can assist you in finding the perfect iPhone 15 for your needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# main \n",
    "async def main():\n",
    "    result = await Runner.run(agent , input = \"Plz help me in shopping..i want obuy a phone iphone 15\",context = comtext)\n",
    "    print(result.final_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c95243",
   "metadata": {},
   "source": [
    "# Output Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1f08cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "from pydantic import BaseModel, EmailStr\n",
    "import asyncio\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9860d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    phoneno : str\n",
    "    email: EmailStr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31001001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name = \"Extract Infor Agent\", \n",
    "    instructions = \"You are a helpfull assistant that can extract user infor from user input\",\n",
    "    model = \"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type = ContactInfo    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98881964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='tayyab ellahi' phoneno='9233345434124' email='tayyab@gmail.com'\n"
     ]
    }
   ],
   "source": [
    "user_input = 'hello i am tayyab ellahi and i m a passionate ai engineer and you can contact me through email tayyab@gmail.com and also contact me from phone 9233345434124'\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent,user_input)\n",
    "    print(result.final_output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd12e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5938bf6",
   "metadata": {},
   "source": [
    "# HandsOff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b0fab",
   "metadata": {},
   "source": [
    "GUARDRAIL IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4d158a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent , Runner, handoff, RunContextWrapper, HandoffInputData, GuardrailFunctionOutput , InputGuardrailTripwireTriggered, OutputGuardrailTripwireTriggered, input_guardrail,  output_guardrail\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from pydantic import BaseModel, EmailStr\n",
    "import asyncio\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "80a8df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
    "print(os.getenv(\"GEMINI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "21b14c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "\n",
    "# if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "#   os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "91649465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response output agent \n",
    "# -------- Pydantic Schemas --------\n",
    "class GuardrailCheckOutput(BaseModel):\n",
    "    is_valid_output: bool\n",
    "    reasoning: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8ca0cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalAnswer(BaseModel):\n",
    "    source_agent: str\n",
    "    user_question: str\n",
    "    final_response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f3fc0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalProblem(BaseModel):\n",
    "    product_id : str\n",
    "    description : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d5285587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BillingProblem(BaseModel):\n",
    "    invoice_no : str\n",
    "    problem_type : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "594fd2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_agent = Agent(\n",
    "    name = \"Guardrial Check\",\n",
    "    instructions= '''\n",
    "    You are a guardrail. Your job is to check if the user's query is related to **customer support**.\n",
    "\n",
    "    Customer support topics include:\n",
    "    - Billing problems\n",
    "    - Technical problems\n",
    "\n",
    "    If the query is related to customer support → set `is_valid_output=true`.\n",
    "    Otherwise (like math questions, chit-chat, unrelated queries) → set `is_valid_output=false`.\n",
    "\n",
    "    Always respond in valid JSON with fields:\n",
    "    {\n",
    "      \"is_valid_output\": true/false,\n",
    "      \"reasoning\": \"your short explanation\"\n",
    "    }\n",
    "    ''',\n",
    "    model = \"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type = GuardrailCheckOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "671ec1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Output Guardrail Agent --------\n",
    "final_answer_guardrail_agent = Agent(\n",
    "    name=\"FinalAnswerGuardrail\",\n",
    "    instructions=f\"\"\"\n",
    "    You are a strict guardrail. Check if the given output is a valid JSON that matches the FinalAnswer format.\n",
    "    The format requires exactly these three string fields: {{ \"source_agent\": \"...\", \"user_question\": \"...\", \"final_response\": \"...\" }}\n",
    "    If it is valid, set is_valid_output=true. Otherwise, set is_valid_output=false.\n",
    "    \"\"\",\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type=GuardrailCheckOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "23c68e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@input_guardrail\n",
    "async def customer_guardrail(ctx: RunContextWrapper , agent :Agent, input:str) -> GuardrailFunctionOutput:\n",
    "    result = await Runner.run(guardrail_agent, input , context = ctx.context)\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info = result.final_output,\n",
    "        tripwire_triggered = not  result.final_output.is_valid_output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a6e8f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output_guardrail\n",
    "async def final_answer_guardrail(ctx: RunContextWrapper, agent: Agent, output: FinalAnswer) -> GuardrailFunctionOutput:\n",
    "    \n",
    "    # result = await Runner.run(final_answer_guardrail_agent, output, context=ctx.context)\n",
    "    output_dict = output.model_dump() if hasattr(output, 'model_dump') else output.dict()\n",
    "    result = await Runner.run(final_answer_guardrail_agent, json.dumps(output_dict), context=ctx.context)\n",
    "    \n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=result.final_output,\n",
    "        tripwire_triggered=not result.final_output.is_valid_output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4b7947fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function input filter function \n",
    "# def billing_input_filter(input:HandoffInputData):   \n",
    "# #     if len(problem.invoice_no) != 6 :\n",
    "# #         raise ValueError(\"your invoice must be 6 digit number\")\n",
    "#     print(input.input_history)\n",
    "#     # Nested dictionary aur list se 'arguments' ki value tak pohunchna\n",
    "#     arguments_value = input['HandoffInputData']['new_items'][0]['raw_item']['arguments']\n",
    "    \n",
    "#     # Value ko print karna\n",
    "#     print(arguments_value)\n",
    "#     return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3c540b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Agents SDK ke zaroori imports yahan honge (farz kar lein)\n",
    "# from agents.runtime import HandoffInputData \n",
    "\n",
    "def billing_input_filter(input: HandoffInputData):\n",
    "    \"\"\"\n",
    "    Yeh filter HandoffInputData object se invoice number nikalta hai\n",
    "    aur uski length ko validate karta hai.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Object ke attributes ko dot (.) se access karein\n",
    "        # 'new_items' ek list/tuple hai, isliye pehla item [0] se lein.\n",
    "        # 'raw_item' ek aur object hai, jiske andar 'arguments' hai.\n",
    "        arguments_string = input.new_items[0].raw_item.arguments\n",
    "        \n",
    "        # 2. 'arguments' ek JSON string hai. Ise Python dictionary mein convert karein.\n",
    "        arguments_dict = json.loads(arguments_string)\n",
    "        \n",
    "        # 3. Dictionary se 'invoice_no' nikalein.\n",
    "        invoice_no = arguments_dict['invoice_no']\n",
    "\n",
    "        print(f\"Invoice number mila: {invoice_no}\") # Debugging ke liye\n",
    "\n",
    "        # 4. Ab aap apna validation logic chala sakte hain.\n",
    "        if len(invoice_no) != 6:\n",
    "            # Agar validation fail ho, to error raise karein. SDK isko handle karega.\n",
    "            raise ValueError(\"Aapka invoice 6 digit ka hona chahiye.\")\n",
    "        \n",
    "        print(\"Invoice number validation pass ho gaya!\")\n",
    "\n",
    "    except (IndexError, KeyError, AttributeError) as e:\n",
    "        # Agar invoice number nahi milta to error raise karein\n",
    "        raise ValueError(f\"Invoice number input data mein nahi mil saka. Error: {e}\")\n",
    "\n",
    "    # Agar sab kuch theek hai, to original input ko wapas return karein.\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0af171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "e3ebbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_handoff_callback_technical(ctx : RunContextWrapper , input: TechnicalProblem):\n",
    "    print( \"Yes the technical Agent is called!!\")\n",
    "    print(f\"Data being passed: {input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TECHNICAL AGENT (FINAL OUTPUT GENERATOR) ---\n",
    "technical_support_agent = Agent(\n",
    "    name=\"Technical Support Agent\",\n",
    "    instructions=f'''\n",
    "    {RECOMMENDED_PROMPT_PREFIX} \n",
    "    You are a helpful Technical Support assistant. The user has been transferred to you.\n",
    "    Your final task is to provide a helpful response and format it as a JSON object of type FinalAnswer.\n",
    "    - Set source_agent to your name: \"Technical Support Agent\".\n",
    "    - Set user_question to the user's original problem.\n",
    "    - Set final_response to your helpful reply.\n",
    "    ''',\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type=FinalAnswer,                # <-- Yahan laga!\n",
    "    output_guardrails=[final_answer_guardrail] # <-- Yahan laga!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7a651159",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_support_handoff = handoff(\n",
    "    agent = technical_support_agent,\n",
    "    tool_name_override = 'Transfer_to_Technical_support',\n",
    "    tool_description_override = \"Transfers a request to the technical support team.\",\n",
    "    on_handoff = on_handoff_callback_technical,\n",
    "    input_type = TechnicalProblem\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "bd0a4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BILLING AGENT (FINAL OUTPUT GENERATOR) ---\n",
    "billing_agent = Agent(\n",
    "    name=\"Billing Agent\",\n",
    "    instructions=f'''\n",
    "    {RECOMMENDED_PROMPT_PREFIX} \n",
    "    You are a helpful billing assistant. The user has been transferred to you.\n",
    "    Your final task is to provide a helpful response and format it as a JSON object of type FinalAnswer.\n",
    "    - Set source_agent to your name: \"Billing Agent\".\n",
    "    - Set user_question to the user's original problem.\n",
    "    - Set final_response to your helpful reply.\n",
    "    ''',\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type=FinalAnswer,                # <-- Yahan bhi laga!\n",
    "    output_guardrails=[final_answer_guardrail] # <-- Yahan bhi laga!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "933b4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_handoff_callback_billing(ctx : RunContextWrapper , input_data: BillingProblem):\n",
    "    print(f\"Yes the billing Agent is called!! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a2b3580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_handoff = handoff(\n",
    "    agent = billing_agent,    \n",
    "    on_handoff = on_handoff_callback_billing,\n",
    "    input_type = BillingProblem,\n",
    "    input_filter = billing_input_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "21baf98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "triage_agent= Agent( \n",
    "    name = \"Triage Agent\", \n",
    "    instructions=f\"\"\" \n",
    "    {RECOMMENDED_PROMPT_PREFIX} \n",
    "   You are a master router. Your job is to analyze the user's query and call the appropriate tool.\n",
    "    \n",
    "    - For technical problems (device issues, hardware/software problems, product malfunctions): call Transfer_to_Technical_Support\n",
    "    - For billing problems (invoice issues, payment problems, billing questions): call Transfer_to_Billing_Support\n",
    "    \n",
    "    Always analyze the query carefully and route to the most appropriate specialist.\n",
    "    Never answer directly - always route to the appropriate specialist team.\n",
    "    \n",
    "    Extract the relevant information:\n",
    "    - For technical issues: extract product_id and description\n",
    "    - For billing issues: extract invoice_no and problem_type\n",
    "   \n",
    "\n",
    "    \"\"\", \n",
    "    model = \"litellm/gemini/gemini-2.5-flash\", \n",
    "    handoffs=[tech_support_handoff,billing_handoff], \n",
    "    input_guardrails=[customer_guardrail]       \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "77206432",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    try:\n",
    "        result = await Runner.run(\n",
    "            triage_agent, \n",
    "            input = \"\"\"\n",
    "        Hello, I have a problem with my phone, product ID 'IPH-123'. \n",
    "        Its charger is not working fine at all.\n",
    "            \"\"\"\n",
    "            )\n",
    "        print(result)\n",
    "    except InputGuardrailTripwireTriggered :\n",
    "        print(\"Bro this is just for customer service only 🚫\")\n",
    "    except OutputGuardrailTripwireTriggered:\n",
    "        print(\"🚫 Output format invalid (math or non-customer support)\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d273fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes the technical Agent is called!!\n",
      "Data being passed: product_id='IPH-123' description='Charger is not working fine at all.'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[290]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[289]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m      4\u001b[39m             triage_agent, \n\u001b[32m      5\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m        Hello, I have a problem with my phone, product ID \u001b[39m\u001b[33m'\u001b[39m\u001b[33mIPH-123\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m        Its charger is not working fine at all.\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m            \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      9\u001b[39m             )\n\u001b[32m     10\u001b[39m         \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m InputGuardrailTripwireTriggered :\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:237\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    236\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    238\u001b[39m     starting_agent,\n\u001b[32m    239\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    240\u001b[39m     context=context,\n\u001b[32m    241\u001b[39m     max_turns=max_turns,\n\u001b[32m    242\u001b[39m     hooks=hooks,\n\u001b[32m    243\u001b[39m     run_config=run_config,\n\u001b[32m    244\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    245\u001b[39m     session=session,\n\u001b[32m    246\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:484\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m generated_items = turn_result.generated_items\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(turn_result.next_step, NextStepFinalOutput):\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     output_guardrail_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_output_guardrails(\n\u001b[32m    485\u001b[39m         current_agent.output_guardrails + (run_config.output_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    486\u001b[39m         current_agent,\n\u001b[32m    487\u001b[39m         turn_result.next_step.output,\n\u001b[32m    488\u001b[39m         context_wrapper,\n\u001b[32m    489\u001b[39m     )\n\u001b[32m    490\u001b[39m     result = RunResult(\n\u001b[32m    491\u001b[39m         \u001b[38;5;28minput\u001b[39m=original_input,\n\u001b[32m    492\u001b[39m         new_items=generated_items,\n\u001b[32m   (...)\u001b[39m\u001b[32m    498\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    499\u001b[39m     )\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# Save the conversation to session if enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:1222\u001b[39m, in \u001b[36mAgentRunner._run_output_guardrails\u001b[39m\u001b[34m(cls, guardrails, agent, agent_output, context)\u001b[39m\n\u001b[32m   1219\u001b[39m guardrail_results = []\n\u001b[32m   1221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m done \u001b[38;5;129;01min\u001b[39;00m asyncio.as_completed(guardrail_tasks):\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m done\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.output.tripwire_triggered:\n\u001b[32m   1224\u001b[39m         \u001b[38;5;66;03m# Cancel all guardrail tasks if a tripwire is triggered.\u001b[39;00m\n\u001b[32m   1225\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m guardrail_tasks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\tasks.py:631\u001b[39m, in \u001b[36mas_completed.<locals>._wait_for_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\_run_impl.py:912\u001b[39m, in \u001b[36mRunImpl.run_single_output_guardrail\u001b[39m\u001b[34m(cls, guardrail, agent, agent_output, context)\u001b[39m\n\u001b[32m    903\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single_output_guardrail\u001b[39m(\n\u001b[32m    905\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    909\u001b[39m     context: RunContextWrapper[TContext],\n\u001b[32m    910\u001b[39m ) -> OutputGuardrailResult:\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m guardrail_span(guardrail.get_name()) \u001b[38;5;28;01mas\u001b[39;00m span_guardrail:\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m guardrail.run(agent=agent, agent_output=agent_output, context=context)\n\u001b[32m    913\u001b[39m         span_guardrail.span_data.triggered = result.output.tripwire_triggered\n\u001b[32m    914\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\guardrail.py:172\u001b[39m, in \u001b[36mOutputGuardrail.run\u001b[39m\u001b[34m(self, context, agent, agent_output)\u001b[39m\n\u001b[32m    166\u001b[39m output = \u001b[38;5;28mself\u001b[39m.guardrail_function(context, agent, agent_output)\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(output):\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m OutputGuardrailResult(\n\u001b[32m    169\u001b[39m         guardrail=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    170\u001b[39m         agent=agent,\n\u001b[32m    171\u001b[39m         agent_output=agent_output,\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m         output=\u001b[38;5;28;01mawait\u001b[39;00m output,\n\u001b[32m    173\u001b[39m     )\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m OutputGuardrailResult(\n\u001b[32m    176\u001b[39m     guardrail=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    177\u001b[39m     agent=agent,\n\u001b[32m    178\u001b[39m     agent_output=agent_output,\n\u001b[32m    179\u001b[39m     output=output,\n\u001b[32m    180\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[271]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mfinal_answer_guardrail\u001b[39m\u001b[34m(ctx, agent, output)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@output_guardrail\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfinal_answer_guardrail\u001b[39m(ctx: RunContextWrapper, agent: Agent, output: FinalAnswer) -> GuardrailFunctionOutput:\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# result = await Runner.run(final_answer_guardrail_agent, output, context=ctx.context)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     output_dict = output.model_dump() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(output, \u001b[33m'\u001b[39m\u001b[33mmodel_dump\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdict\u001b[49m()\n\u001b[32m      6\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(final_answer_guardrail_agent, json.dumps(output_dict), context=ctx.context)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GuardrailFunctionOutput(\n\u001b[32m      9\u001b[39m         output_info=result.final_output,\n\u001b[32m     10\u001b[39m         tripwire_triggered=\u001b[38;5;129;01mnot\u001b[39;00m result.final_output.is_valid_output\n\u001b[32m     11\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'dict'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    await main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd379adc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "796feaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHello, i need help i want to tell u i have problem in my phone\\n        its charger is not working fine\\n\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Hello, i need help i want to tell u i have problem in my phone\n",
    "        its charger is not working fine\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "808da768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting customer support workflow...\n",
      "✅ Technical Agent called!\n",
      "Data being passed: product_id='IPH-123' description='Its charger is not working fine at all.'\n",
      "✅ Workflow completed successfully!\n",
      "📋 Final Result:\n",
      "RunResult:\n",
      "- Last agent: Agent(name=\"Technical Support Agent\", ...)\n",
      "- Final output (FinalAnswer):\n",
      "    {\n",
      "      \"source_agent\": \"Technical Support Agent\",\n",
      "      \"user_question\": \"Hello, I have a problem with my phone, product ID 'IPH-123'. Its charger is not working fine at all.\",\n",
      "      \"final_response\": \"I'm sorry to hear you're having trouble with your charger for product ID 'IPH-123'. Let's try to troubleshoot this. First, please ensure the charger is securely plugged into both the wall outlet and your phone. If it's still not working, try plugging the charger into a different wall outlet to rule out a power source issue. You could also try using a different cable with the same charging adapter, or a different adapter with the same cable, if you have spares available. This helps us pinpoint if the issue is with the cable, the adapter, or the phone itself. If none of these steps resolve the issue, please provide more details on what happens when you try to charge (e.g., no light, intermittent charging, etc.), and we can explore further options, including potential replacement or repair.\"\n",
      "    }\n",
      "- 3 new item(s)\n",
      "- 2 raw response(s)\n",
      "- 1 input guardrail result(s)\n",
      "- 1 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, handoff, RunContextWrapper, HandoffInputData, GuardrailFunctionOutput, InputGuardrailTripwireTriggered, OutputGuardrailTripwireTriggered, input_guardrail, output_guardrail\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from pydantic import BaseModel, EmailStr\n",
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "\n",
    "# -------- Pydantic Schemas --------\n",
    "class GuardrailCheckOutput(BaseModel):\n",
    "    is_valid_output: bool\n",
    "    reasoning: str\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    source_agent: str\n",
    "    user_question: str\n",
    "    final_response: str\n",
    "\n",
    "class TechnicalProblem(BaseModel):\n",
    "    product_id: str\n",
    "    description: str\n",
    "\n",
    "class BillingProblem(BaseModel):\n",
    "    invoice_no: str\n",
    "    problem_type: str\n",
    "\n",
    "# -------- Guardrail Agents --------\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail Check\",\n",
    "    instructions='''\n",
    "    You are a guardrail. Your job is to check if the user's query is related to **customer support**.\n",
    "\n",
    "    Customer support topics include:\n",
    "    - Billing problems\n",
    "    - Technical problems\n",
    "    - Product issues\n",
    "    - Support requests\n",
    "\n",
    "    If the query is related to customer support → set `is_valid_output=true`.\n",
    "    Otherwise (like math questions, chit-chat, unrelated queries) → set `is_valid_output=false`.\n",
    "\n",
    "    Always respond in valid JSON with fields:\n",
    "    {\n",
    "      \"is_valid_output\": true/false,\n",
    "      \"reasoning\": \"your short explanation\"\n",
    "    }\n",
    "    ''',\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type=GuardrailCheckOutput\n",
    ")\n",
    "\n",
    "final_answer_guardrail_agent = Agent(\n",
    "    name=\"FinalAnswerGuardrail\",\n",
    "    instructions=f\"\"\"\n",
    "    You are a strict guardrail. Check if the given output is a valid JSON that matches the FinalAnswer format.\n",
    "    The format requires exactly these three string fields: {{ \"source_agent\": \"...\", \"user_question\": \"...\", \"final_response\": \"...\" }}\n",
    "    If it is valid, set is_valid_output=true. Otherwise, set is_valid_output=false.\n",
    "    \"\"\",\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type=GuardrailCheckOutput\n",
    ")\n",
    "\n",
    "# -------- Guardrail Functions --------\n",
    "@input_guardrail\n",
    "async def customer_guardrail(ctx: RunContextWrapper, agent: Agent, input: str) -> GuardrailFunctionOutput:\n",
    "    try:\n",
    "        result = await Runner.run(guardrail_agent, input, context=ctx.context)\n",
    "        return GuardrailFunctionOutput(\n",
    "            output_info=result.final_output,\n",
    "            tripwire_triggered=not result.final_output.is_valid_output\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Guardrail error: {e}\")\n",
    "        # Default to allowing customer support queries if guardrail fails\n",
    "        return GuardrailFunctionOutput(\n",
    "            output_info=GuardrailCheckOutput(is_valid_output=True, reasoning=\"Guardrail failed, defaulting to allow\"),\n",
    "            tripwire_triggered=False\n",
    "        )\n",
    "\n",
    "@output_guardrail\n",
    "async def final_answer_guardrail(ctx: RunContextWrapper, agent: Agent, output: FinalAnswer) -> GuardrailFunctionOutput:\n",
    "    try:\n",
    "        # Convert FinalAnswer to dict for validation\n",
    "        output_dict = output.model_dump() if hasattr(output, 'model_dump') else output.dict()\n",
    "        result = await Runner.run(final_answer_guardrail_agent, json.dumps(output_dict), context=ctx.context)\n",
    "        return GuardrailFunctionOutput(\n",
    "            output_info=result.final_output,\n",
    "            tripwire_triggered=not result.final_output.is_valid_output\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Output guardrail error: {e}\")\n",
    "        # Default to allowing valid FinalAnswer objects\n",
    "        return GuardrailFunctionOutput(\n",
    "            output_info=GuardrailCheckOutput(is_valid_output=True, reasoning=\"Output guardrail failed, defaulting to allow\"),\n",
    "            tripwire_triggered=False\n",
    "        )\n",
    "\n",
    "# -------- Input Filter Functions --------\n",
    "def billing_input_filter(input_data: HandoffInputData):\n",
    "    \"\"\"\n",
    "    Filter to validate billing input data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the arguments from HandoffInputData\n",
    "        if hasattr(input_data, 'new_items') and len(input_data.new_items) > 0:\n",
    "            # Get the first item's arguments\n",
    "            arguments_string = input_data.new_items[0].raw_item.arguments\n",
    "            arguments_dict = json.loads(arguments_string)\n",
    "            invoice_no = arguments_dict.get('invoice_no', '')\n",
    "            \n",
    "            print(f\"Invoice number received: {invoice_no}\")\n",
    "            \n",
    "            # Validate invoice number length\n",
    "            if len(invoice_no) != 6:\n",
    "                raise ValueError(\"Invoice number must be exactly 6 digits long.\")\n",
    "            \n",
    "            print(\"Invoice number validation passed!\")\n",
    "            \n",
    "        return input_data\n",
    "        \n",
    "    except (IndexError, KeyError, AttributeError, json.JSONDecodeError) as e:\n",
    "        print(f\"Input filter error: {e}\")\n",
    "        raise ValueError(f\"Invalid invoice data format. Error: {e}\")\n",
    "\n",
    "# -------- Callback Functions --------\n",
    "def on_handoff_callback_technical(ctx: RunContextWrapper, input_data: TechnicalProblem):\n",
    "    print(\"✅ Technical Agent called!\")\n",
    "    print(f\"Data being passed: {input_data}\")\n",
    "\n",
    "def on_handoff_callback_billing(ctx: RunContextWrapper, input_data: BillingProblem):\n",
    "    print(\"✅ Billing Agent called!\")\n",
    "    print(f\"Data being passed: {input_data}\")\n",
    "\n",
    "# -------- Agent Definitions --------\n",
    "technical_support_agent = Agent(\n",
    "    name=\"Technical Support Agent\",\n",
    "    instructions=f'''\n",
    "    {RECOMMENDED_PROMPT_PREFIX}\n",
    "    You are a helpful Technical Support assistant. The user has been transferred to you.\n",
    "    Your task is to provide a helpful response and format it as a JSON object of type FinalAnswer.\n",
    "    - Set source_agent to your name: \"Technical Support Agent\"\n",
    "    - Set user_question to the user's original problem\n",
    "    - Set final_response to your helpful technical support reply\n",
    "    \n",
    "    Provide clear, actionable solutions for technical problems.\n",
    "    ''',\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type=FinalAnswer,\n",
    "    output_guardrails=[final_answer_guardrail]\n",
    ")\n",
    "\n",
    "billing_agent = Agent(\n",
    "    name=\"Billing Agent\",\n",
    "    instructions=f'''\n",
    "    {RECOMMENDED_PROMPT_PREFIX}\n",
    "    You are a helpful billing assistant. The user has been transferred to you.\n",
    "    Your task is to provide a helpful response and format it as a JSON object of type FinalAnswer.\n",
    "    - Set source_agent to your name: \"Billing Agent\"\n",
    "    - Set user_question to the user's original problem\n",
    "    - Set final_response to your helpful billing support reply\n",
    "    \n",
    "    Help users with billing issues, payment problems, and invoice questions.\n",
    "    ''',\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    output_type=FinalAnswer,\n",
    "    output_guardrails=[final_answer_guardrail]\n",
    ")\n",
    "\n",
    "# -------- Handoff Definitions --------\n",
    "tech_support_handoff = handoff(\n",
    "    agent=technical_support_agent,\n",
    "    tool_name_override='Transfer_to_Technical_Support',\n",
    "    tool_description_override=\"Transfers a request to the technical support team for hardware/software issues.\",\n",
    "    on_handoff=on_handoff_callback_technical,\n",
    "    input_type=TechnicalProblem\n",
    ")\n",
    "\n",
    "billing_handoff = handoff(\n",
    "    agent=billing_agent,\n",
    "    tool_name_override='Transfer_to_Billing_Support',\n",
    "    tool_description_override=\"Transfers a request to the billing team for payment and invoice issues.\",\n",
    "    on_handoff=on_handoff_callback_billing,\n",
    "    input_type=BillingProblem,\n",
    "    input_filter=billing_input_filter\n",
    ")\n",
    "\n",
    "# -------- Triage Agent --------\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=f'''\n",
    "    {RECOMMENDED_PROMPT_PREFIX}\n",
    "    You are a master router. Your job is to analyze the user's query and call the appropriate tool.\n",
    "    \n",
    "    - For technical problems (device issues, hardware/software problems, product malfunctions): call Transfer_to_Technical_Support\n",
    "    - For billing problems (invoice issues, payment problems, billing questions): call Transfer_to_Billing_Support\n",
    "    \n",
    "    Always analyze the query carefully and route to the most appropriate specialist.\n",
    "    Never answer directly - always route to the appropriate specialist team.\n",
    "    \n",
    "    Extract the relevant information:\n",
    "    - For technical issues: extract product_id and description\n",
    "    - For billing issues: extract invoice_no and problem_type\n",
    "    ''',\n",
    "    model=\"litellm/gemini/gemini-2.5-flash\",\n",
    "    handoffs=[tech_support_handoff, billing_handoff],\n",
    "    input_guardrails=[customer_guardrail]\n",
    ")\n",
    "\n",
    "# -------- Main Function --------\n",
    "async def main():\n",
    "    try:\n",
    "        print(\"🔄 Starting customer support workflow...\")\n",
    "        \n",
    "        result = await Runner.run(\n",
    "            triage_agent,\n",
    "            input=\"\"\"\n",
    "            Hello, I have a problem with my phone, product ID 'IPH-123'. \n",
    "            Its charger is not working fine at all.\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Workflow completed successfully!\")\n",
    "        print(\"📋 Final Result:\")\n",
    "        print(result)\n",
    "        \n",
    "    except InputGuardrailTripwireTriggered as e:\n",
    "        print(\"🚫 Input blocked: This service is only for customer support queries.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        \n",
    "    except OutputGuardrailTripwireTriggered as e:\n",
    "        print(\"🚫 Output blocked: Invalid response format detected.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error occurred: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# -------- Execution --------\n",
    "if __name__ == '__main__':\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872b5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f62ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1eb0a15",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "4261a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "Agent updated: Joker\n",
      "-- Tool was called\n",
      "-- Tool output: 10\n",
      "-- Message output:\n",
      " Here are 10 jokes for you!\n",
      "\n",
      "1. Why don't scientists trust atoms? Because they make up everything!\n",
      "2. What do you call a fake noodle? An impasta!\n",
      "3. Why did the scarecrow win an award? Because he was outstanding in his field!\n",
      "4. What do you call a cheese that isn't yours? Nacho cheese!\n",
      "5. Why don't skeletons fight each other? They don't have the guts!\n",
      "6. What's orange and sounds like a parrot? A carrot!\n",
      "7. What do you call a boomerang that won't come back? A stick!\n",
      "8. Why did the bicycle fall over? Because it was two tired!\n",
      "9. What did the grape say when it got stepped on? Nothing, it just let out a little wine!\n",
      "10. How do you organize a space party? You \"planet\"!\n",
      "=== Run complete ===\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    return random.randint(1, 10)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
    "        tools=[how_many_jokes],\n",
    "        model = \"litellm/gemini/gemini-2.5-flash\"\n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"Hello\",\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events():\n",
    "        # We'll ignore the raw responses event deltas\n",
    "        if event.type == \"raw_response_event\":\n",
    "            continue\n",
    "        # When the agent updates, print that\n",
    "        elif event.type == \"agent_updated_stream_event\":\n",
    "            print(f\"Agent updated: {event.new_agent.name}\")\n",
    "            continue\n",
    "        # When items are generated, print them\n",
    "        elif event.type == \"run_item_stream_event\":\n",
    "            if event.item.type == \"tool_call_item\":\n",
    "                print(\"-- Tool was called\")\n",
    "            elif event.item.type == \"tool_call_output_item\":\n",
    "                print(f\"-- Tool output: {event.item.output}\")\n",
    "            elif event.item.type == \"message_output_item\":\n",
    "                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
    "            else:\n",
    "                pass  # Ignore other event types\n",
    "\n",
    "    print(\"=== Run complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await (main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb6294a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "MidStreamFallbackError",
     "evalue": "litellm.ServiceUnavailableError: litellm.MidStreamFallbackError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\",\\n    \"details\": [\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\\n        \"violations\": [\\n          {\\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\\n            \"quotaDimensions\": {\\n              \"location\": \"global\",\\n              \"model\": \"gemini-2.5-flash\"\\n            },\\n            \"quotaValue\": \"250\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\\n        \"links\": [\\n          {\\n            \"description\": \"Learn more about Gemini API quotas\",\\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\\n        \"retryDelay\": \"47s\"\\n      }\\n    ]\\n  }\\n}\\n' Original exception: RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\",\\n    \"details\": [\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\\n        \"violations\": [\\n          {\\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\\n            \"quotaDimensions\": {\\n              \"location\": \"global\",\\n              \"model\": \"gemini-2.5-flash\"\\n            },\\n            \"quotaValue\": \"250\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\\n        \"links\": [\\n          {\\n            \"description\": \"Learn more about Gemini API quotas\",\\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\\n        \"retryDelay\": \"47s\"\\n      }\\n    ]\\n  }\\n}\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py:1452\u001b[39m, in \u001b[36mmake_call\u001b[39m\u001b[34m(client, api_base, headers, data, model, messages, logging_obj)\u001b[39m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client.post(api_base, headers=headers, data=data, stream=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1453\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\logging_utils.py:135\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:324\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:280\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    279\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=AIzaSyDQ5UCL6CHe_-FbtSkDDuvpUE8mrF_FCTg&alt=sse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mVertexAIError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\streaming_handler.py:1689\u001b[39m, in \u001b[36mCustomStreamWrapper.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1688\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.completion_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1689\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fetch_stream()\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_async_iterable(\u001b[38;5;28mself\u001b[39m.completion_stream):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\streaming_handler.py:1673\u001b[39m, in \u001b[36mCustomStreamWrapper.fetch_stream\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.completion_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1672\u001b[39m     \u001b[38;5;66;03m# Call make_call to get the completion stream\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m     \u001b[38;5;28mself\u001b[39m.completion_stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_call(\n\u001b[32m   1674\u001b[39m         client=litellm.module_level_aclient\n\u001b[32m   1675\u001b[39m     )\n\u001b[32m   1676\u001b[39m     \u001b[38;5;28mself\u001b[39m._stream_iter = \u001b[38;5;28mself\u001b[39m.completion_stream.\u001b[34m__aiter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py:1456\u001b[39m, in \u001b[36mmake_call\u001b[39m\u001b[34m(client, api_base, headers, data, model, messages, logging_obj)\u001b[39m\n\u001b[32m   1455\u001b[39m     exception_string = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m e.response.aread())\n\u001b[32m-> \u001b[39m\u001b[32m1456\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[32m   1457\u001b[39m         status_code=e.response.status_code,\n\u001b[32m   1458\u001b[39m         message=VertexGeminiConfig().translate_exception_str(exception_string),\n\u001b[32m   1459\u001b[39m         headers=e.response.headers,\n\u001b[32m   1460\u001b[39m     )\n\u001b[32m   1461\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m response.status_code != \u001b[32m201\u001b[39m:\n",
      "\u001b[31mVertexAIError\u001b[39m: b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\",\\n    \"details\": [\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\\n        \"violations\": [\\n          {\\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\\n            \"quotaDimensions\": {\\n              \"location\": \"global\",\\n              \"model\": \"gemini-2.5-flash\"\\n            },\\n            \"quotaValue\": \"250\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\\n        \"links\": [\\n          {\\n            \"description\": \"Learn more about Gemini API quotas\",\\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\\n        \"retryDelay\": \"47s\"\\n      }\\n    ]\\n  }\\n}\\n'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\streaming_handler.py:1871\u001b[39m, in \u001b[36mCustomStreamWrapper.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1870\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1871\u001b[39m     \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1874\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1878\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2301\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:1330\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m   1331\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlitellm.RateLimitError: VertexAIException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1332\u001b[39m         model=model,\n\u001b[32m   1333\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33mvertex_ai\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1334\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   1335\u001b[39m         response=httpx.Response(\n\u001b[32m   1336\u001b[39m             status_code=\u001b[32m429\u001b[39m,\n\u001b[32m   1337\u001b[39m             request=httpx.Request(\n\u001b[32m   1338\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1339\u001b[39m                 url=\u001b[33m\"\u001b[39m\u001b[33m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1340\u001b[39m             ),\n\u001b[32m   1341\u001b[39m         ),\n\u001b[32m   1342\u001b[39m     )\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_exception.status_code == \u001b[32m500\u001b[39m:\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\",\\n    \"details\": [\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\\n        \"violations\": [\\n          {\\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\\n            \"quotaDimensions\": {\\n              \"location\": \"global\",\\n              \"model\": \"gemini-2.5-flash\"\\n            },\\n            \"quotaValue\": \"250\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\\n        \"links\": [\\n          {\\n            \"description\": \"Learn more about Gemini API quotas\",\\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\\n        \"retryDelay\": \"47s\"\\n      }\\n    ]\\n  }\\n}\\n'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mMidStreamFallbackError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m             \u001b[38;5;28mprint\u001b[39m(event.data.delta, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m (main())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m run_config = RunConfig(\n\u001b[32m     10\u001b[39m     model = \u001b[33m\"\u001b[39m\u001b[33mlitellm/gemini/gemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     workflow_name = \u001b[33m'\u001b[39m\u001b[33mAgent workflow\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     trace_id = \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m     group_id = \u001b[33m'\u001b[39m\u001b[33mg1\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m result = Runner.run_streamed(agent, \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mwrite a 500 lines essay\u001b[39m\u001b[33m\"\u001b[39m, run_config = run_config)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m result.stream_events():\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event.type == \u001b[33m\"\u001b[39m\u001b[33mraw_response_event\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event.data, ResponseTextDeltaEvent):\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(event.data.delta, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\result.py:215\u001b[39m, in \u001b[36mRunResultStreaming.stream_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m._cleanup_tasks()\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:795\u001b[39m, in \u001b[36mAgentRunner._start_streaming\u001b[39m\u001b[34m(cls, starting_input, streamed_result, starting_agent, max_turns, hooks, context_wrapper, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    784\u001b[39m     streamed_result._input_guardrails_task = asyncio.create_task(\n\u001b[32m    785\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails_with_queue(\n\u001b[32m    786\u001b[39m             starting_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m    792\u001b[39m         )\n\u001b[32m    793\u001b[39m     )\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn_streamed(\n\u001b[32m    796\u001b[39m         streamed_result,\n\u001b[32m    797\u001b[39m         current_agent,\n\u001b[32m    798\u001b[39m         hooks,\n\u001b[32m    799\u001b[39m         context_wrapper,\n\u001b[32m    800\u001b[39m         run_config,\n\u001b[32m    801\u001b[39m         should_run_agent_start_hooks,\n\u001b[32m    802\u001b[39m         tool_use_tracker,\n\u001b[32m    803\u001b[39m         all_tools,\n\u001b[32m    804\u001b[39m         previous_response_id,\n\u001b[32m    805\u001b[39m     )\n\u001b[32m    806\u001b[39m     should_run_agent_start_hooks = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    808\u001b[39m     streamed_result.raw_responses = streamed_result.raw_responses + [\n\u001b[32m    809\u001b[39m         turn_result.model_response\n\u001b[32m    810\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\run.py:954\u001b[39m, in \u001b[36mAgentRunner._run_single_turn_streamed\u001b[39m\u001b[34m(cls, streamed_result, agent, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, all_tools, previous_response_id)\u001b[39m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m agent.hooks.on_llm_start(\n\u001b[32m    950\u001b[39m         context_wrapper, agent, filtered.instructions, filtered.input\n\u001b[32m    951\u001b[39m     )\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# 1. Stream the output events\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m model.stream_response(\n\u001b[32m    955\u001b[39m     filtered.instructions,\n\u001b[32m    956\u001b[39m     filtered.input,\n\u001b[32m    957\u001b[39m     model_settings,\n\u001b[32m    958\u001b[39m     all_tools,\n\u001b[32m    959\u001b[39m     output_schema,\n\u001b[32m    960\u001b[39m     handoffs,\n\u001b[32m    961\u001b[39m     get_model_tracing_impl(\n\u001b[32m    962\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    963\u001b[39m     ),\n\u001b[32m    964\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    965\u001b[39m     prompt=prompt_config,\n\u001b[32m    966\u001b[39m ):\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, ResponseCompletedEvent):\n\u001b[32m    968\u001b[39m         usage = (\n\u001b[32m    969\u001b[39m             Usage(\n\u001b[32m    970\u001b[39m                 requests=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    978\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Usage()\n\u001b[32m    979\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\extensions\\models\\litellm_model.py:197\u001b[39m, in \u001b[36mLitellmModel.stream_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[39m\n\u001b[32m    183\u001b[39m response, stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    184\u001b[39m     system_instructions,\n\u001b[32m    185\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     prompt=prompt,\n\u001b[32m    194\u001b[39m )\n\u001b[32m    196\u001b[39m final_response: Response | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m ChatCmplStreamHandler.handle_stream(response, stream):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.type == \u001b[33m\"\u001b[39m\u001b[33mresponse.completed\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\agents\\models\\chatcmpl_stream_handler.py:81\u001b[39m, in \u001b[36mChatCmplStreamHandler.handle_stream\u001b[39m\u001b[34m(cls, response, stream)\u001b[39m\n\u001b[32m     79\u001b[39m state = StreamingState()\n\u001b[32m     80\u001b[39m sequence_number = SequenceNumber()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state.started:\n\u001b[32m     83\u001b[39m         state.started = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\litellm\\litellm_core_utils\\streaming_handler.py:1881\u001b[39m, in \u001b[36mCustomStreamWrapper.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1878\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1879\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MidStreamFallbackError\n\u001b[32m-> \u001b[39m\u001b[32m1881\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MidStreamFallbackError(\n\u001b[32m   1882\u001b[39m         message=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   1883\u001b[39m         model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m   1884\u001b[39m         llm_provider=\u001b[38;5;28mself\u001b[39m.custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33manthropic\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1885\u001b[39m         original_exception=e,\n\u001b[32m   1886\u001b[39m         generated_content=\u001b[38;5;28mself\u001b[39m.response_uptil_now,\n\u001b[32m   1887\u001b[39m         is_pre_first_chunk=\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sent_first_chunk,\n\u001b[32m   1888\u001b[39m     )\n",
      "\u001b[31mMidStreamFallbackError\u001b[39m: litellm.ServiceUnavailableError: litellm.MidStreamFallbackError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\",\\n    \"details\": [\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\\n        \"violations\": [\\n          {\\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\\n            \"quotaDimensions\": {\\n              \"location\": \"global\",\\n              \"model\": \"gemini-2.5-flash\"\\n            },\\n            \"quotaValue\": \"250\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\\n        \"links\": [\\n          {\\n            \"description\": \"Learn more about Gemini API quotas\",\\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\\n        \"retryDelay\": \"47s\"\\n      }\\n    ]\\n  }\\n}\\n' Original exception: RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\",\\n    \"details\": [\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\\n        \"violations\": [\\n          {\\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\\n            \"quotaDimensions\": {\\n              \"location\": \"global\",\\n              \"model\": \"gemini-2.5-flash\"\\n            },\\n            \"quotaValue\": \"250\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\\n        \"links\": [\\n          {\\n            \"description\": \"Learn more about Gemini API quotas\",\\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n          }\\n        ]\\n      },\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\\n        \"retryDelay\": \"47s\"\\n      }\\n    ]\\n  }\\n}\\n'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner,RunConfig\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "    )\n",
    "    run_config = RunConfig(\n",
    "        model = \"litellm/gemini/gemini-2.5-flash\",\n",
    "        workflow_name = 'Agent workflow',\n",
    "        trace_id = '1',\n",
    "        group_id = 'g1'\n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(agent, input=\"write a 500 lines essay\", run_config = run_config)\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await (main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82775da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2951159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "import asyncio\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name = \"Assitant\",\n",
    "        instructions= \"You are a helpful assistant\",\n",
    "        model= \"litellm/gemini/gemini-2.5-flash\"\n",
    "    )\n",
    "    result = Runner.run_streamed(agent,input=\"Write a essay on Imran khan of 500 words\")\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event)\n",
    "            print(event.type)\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    await (main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff51f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
